{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "579f4ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.pip_utils import configure_spark_with_delta_pip\n",
    "\n",
    "from utils.string_utils import camel_to_snake, normalize_string\n",
    "from engine import SupermetricsAPI\n",
    "from etl import default, get_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b30e3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sm_normalize_cols(s):\n",
    "    s = normalize_string(s)\n",
    "    s = camel_to_snake(s)\n",
    "    s = s.replace(' ', '_')\n",
    "    s = re.sub(r'_+', '_', s)\n",
    "    s = s.strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ff63af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: delta.enableChangeDataFeed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/23 20:12:22 WARN Utils: Your hostname, spiriel resolves to a loopback address: 127.0.1.1; using 192.168.15.6 instead (on interface enp3s0)\n",
      "23/04/23 20:12:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/ahow/main_env/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ahow/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ahow/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d19a7efd-4593-4da1-b862-2e267004e8e2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.2.0 in central\n",
      "\tfound io.delta#delta-storage;2.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 90ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d19a7efd-4593-4da1-b862-2e267004e8e2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/23 20:12:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = SparkSession.builder \\\n",
    "    .appName('test_delta_utils') \\\n",
    "    .config('spark.sql.warehouse.dir', 'pyspark_tables') \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config('spark.databricks.delta.retentionDurationCheck.enabled', False) \\\n",
    "    .config('spark.databricks.delta.schema.autoMerge.enabled', True) \\\n",
    "    .config('spark.databricks.delta.checkLatestSchemaOnRead', True) \\\n",
    "    .config(\"spark.log.level\", \"ERROR\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .config('delta.enableChangeDataFeed', True)\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).enableHiveSupport().getOrCreate()\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS supermetrics\")\n",
    "\n",
    "TB = 'supermetrics.test1'\n",
    "URL = os.environ['URL']\n",
    "PK = ['Sheet', 'Column A']\n",
    "PARTITION = ['Sheet']\n",
    "DT_START = datetime(2022, 1, 1)\n",
    "DT_END = datetime(2022, 3, 1)\n",
    "DT_DELTA = timedelta(days=15)\n",
    "SM_KWARGS = {'offset': 100000}\n",
    "R_KWARGS = {'timeout': 600}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "619339f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "*********** Starting SM ETL **********\n",
      "**************************************\n",
      "from 01/01/2022 to 15/01/2022\n",
      "extracting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging\n",
      "23/04/23 20:12:42 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "from 16/01/2022 to 30/01/2022\n",
      "extracting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging\n",
      "\n",
      "\n",
      "from 31/01/2022 to 14/02/2022\n",
      "extracting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging\n",
      "\n",
      "\n",
      "from 15/02/2022 to 28/02/2022\n",
      "extracting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = get_df(api, spark, dt_start, dt_end, r_kwargs=r_kwargs, replace_f=[sm_normalize_cols])\n",
    "# df.show()\n",
    "\n",
    "api = SupermetricsAPI(URL, **SM_KWARGS)\n",
    "\n",
    "default(\n",
    "    api, spark, TB, PK, PARTITION,\n",
    "    DT_START, DT_END, DT_DELTA,\n",
    "    r_kwargs=R_KWARGS, replace_f=[sm_normalize_cols]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "073b3d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sheet</th>\n",
       "      <th>column_a</th>\n",
       "      <th>column_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sheet1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sheet1</td>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheet1</td>\n",
       "      <td>3</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sheet1</td>\n",
       "      <td>4</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sheet1</td>\n",
       "      <td>5</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sheet1</td>\n",
       "      <td>6</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sheet1</td>\n",
       "      <td>7</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sheet1</td>\n",
       "      <td>8</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sheet2</td>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sheet2</td>\n",
       "      <td>3</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sheet2</td>\n",
       "      <td>4</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sheet2</td>\n",
       "      <td>5</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sheet2</td>\n",
       "      <td>6</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sheet2</td>\n",
       "      <td>7</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sheet2</td>\n",
       "      <td>8</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sheet column_a column_b\n",
       "0   Sheet1        1        a\n",
       "1   Sheet1        2        b\n",
       "2   Sheet1        3        c\n",
       "3   Sheet1        4        d\n",
       "4   Sheet1        5        e\n",
       "5   Sheet1        6        f\n",
       "6   Sheet1        7        g\n",
       "7   Sheet1        8        h\n",
       "8   sheet2        2        b\n",
       "9   sheet2        3        c\n",
       "10  sheet2        4        d\n",
       "11  sheet2        5        e\n",
       "12  sheet2        6        f\n",
       "13  sheet2        7        g\n",
       "14  sheet2        8        h"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "SELECT * FROM {TB}\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52dfd7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "      <th>engineInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-04-23 20:12:57.412</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MERGE</td>\n",
       "      <td>{'matchedPredicates': '[{\"actionType\":\"update\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '15', 'numTargetRowsInserted...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-04-23 20:12:54.206</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MERGE</td>\n",
       "      <td>{'matchedPredicates': '[{\"actionType\":\"update\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '15', 'numTargetRowsInserted...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-04-23 20:12:50.953</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MERGE</td>\n",
       "      <td>{'matchedPredicates': '[{\"actionType\":\"update\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '15', 'numTargetRowsInserted...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-04-23 20:12:47.596</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MERGE</td>\n",
       "      <td>{'matchedPredicates': '[{\"actionType\":\"update\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '15', 'numTargetRowsInserted...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-04-18 01:21:18.863</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MERGE</td>\n",
       "      <td>{'matchedPredicates': '[{\"actionType\":\"update\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '14', 'numTargetRowsInserted...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-04-18 01:11:13.483</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CREATE OR REPLACE TABLE AS SELECT</td>\n",
       "      <td>{'description': None, 'partitionBy': '[\"sheet\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '14', 'numOutputBytes': '820...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName  \\\n",
       "0        5 2023-04-23 20:12:57.412   None     None   \n",
       "1        4 2023-04-23 20:12:54.206   None     None   \n",
       "2        3 2023-04-23 20:12:50.953   None     None   \n",
       "3        2 2023-04-23 20:12:47.596   None     None   \n",
       "4        1 2023-04-18 01:21:18.863   None     None   \n",
       "5        0 2023-04-18 01:11:13.483   None     None   \n",
       "\n",
       "                           operation  \\\n",
       "0                              MERGE   \n",
       "1                              MERGE   \n",
       "2                              MERGE   \n",
       "3                              MERGE   \n",
       "4                              MERGE   \n",
       "5  CREATE OR REPLACE TABLE AS SELECT   \n",
       "\n",
       "                                 operationParameters   job notebook clusterId  \\\n",
       "0  {'matchedPredicates': '[{\"actionType\":\"update\"...  None     None      None   \n",
       "1  {'matchedPredicates': '[{\"actionType\":\"update\"...  None     None      None   \n",
       "2  {'matchedPredicates': '[{\"actionType\":\"update\"...  None     None      None   \n",
       "3  {'matchedPredicates': '[{\"actionType\":\"update\"...  None     None      None   \n",
       "4  {'matchedPredicates': '[{\"actionType\":\"update\"...  None     None      None   \n",
       "5  {'description': None, 'partitionBy': '[\"sheet\"...  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend  \\\n",
       "0          4.0   Serializable          False   \n",
       "1          3.0   Serializable          False   \n",
       "2          2.0   Serializable          False   \n",
       "3          1.0   Serializable          False   \n",
       "4          0.0   Serializable          False   \n",
       "5          NaN   Serializable          False   \n",
       "\n",
       "                                    operationMetrics userMetadata  \\\n",
       "0  {'numOutputRows': '15', 'numTargetRowsInserted...         None   \n",
       "1  {'numOutputRows': '15', 'numTargetRowsInserted...         None   \n",
       "2  {'numOutputRows': '15', 'numTargetRowsInserted...         None   \n",
       "3  {'numOutputRows': '15', 'numTargetRowsInserted...         None   \n",
       "4  {'numOutputRows': '14', 'numTargetRowsInserted...         None   \n",
       "5  {'numOutputRows': '14', 'numOutputBytes': '820...         None   \n",
       "\n",
       "                            engineInfo  \n",
       "0  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "1  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "2  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "3  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "4  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "5  Apache-Spark/3.3.1 Delta-Lake/2.2.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "DESCRIBE HISTORY {TB}\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4accc00e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

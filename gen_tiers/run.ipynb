{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3118170c",
   "metadata": {},
   "source": [
    "# This script implements tiers of tables using streaming to pass data from one tier to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2847fd2a",
   "metadata": {},
   "source": [
    "## Start pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ba6ed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/22 18:08:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# import and enable spark\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, LongType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import import_ipynb\n",
    "from Bronze import BronzeETL\n",
    "\n",
    "from delta.pip_utils import configure_spark_with_delta_pip\n",
    "    \n",
    "builder = SparkSession.builder\\\n",
    "       .appName('raw_etl')\\\n",
    "       .config('spark.sql.warehouse.dir', 'pyspark_tables')\\\n",
    "       .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "       .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "       .config('spark.databricks.delta.retentionDurationCheck.enabled', False) \\\n",
    "       .config('spark.databricks.delta.schema.autoMerge.enabled', True) \\\n",
    "       .config('spark.databricks.delta.checkLatestSchemaOnRead', True) \\\n",
    "       .config('delta.enableChangeDataFeed', True) \\\n",
    "       .config('spark.sql.shuffle.partitions', 10) \\\n",
    "       .config('spark.databricks.preemption.enabled', True) \\\n",
    "       .config('spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite', True)\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7527ed",
   "metadata": {},
   "source": [
    "## Clear previuous tables from other runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ad6caa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/22 18:26:43 ERROR MicroBatchExecution: Query [id = e7e11fdb-754b-4482-9a8f-33b802d298da, runId = 5e9003da-5d59-4e3c-80f6-9c40c244e78a] terminated with error\n",
      "java.io.FileNotFoundException: No such file or directory: file:/home/ahow/MyGitHub/pyspark_tests/gen_tiers/pyspark_tables/etl_tiers.db/raw/_delta_log\n",
      "\tat io.delta.storage.HadoopFileSystemLogStore.listFrom(HadoopFileSystemLogStore.java:56)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreAdaptor.listFrom(LogStore.scala:452)\n",
      "\tat org.apache.spark.sql.delta.storage.DelegatingLogStore.listFrom(DelegatingLogStore.scala:127)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.getChanges(DeltaLog.scala:297)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSourceCDCSupport.filterAndIndexDeltaLogs$1(DeltaSourceCDCSupport.scala:205)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSourceCDCSupport.getFileChangesForCDC(DeltaSourceCDCSupport.scala:260)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSourceCDCSupport.getFileChangesForCDC$(DeltaSourceCDCSupport.scala:195)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.getFileChangesForCDC(DeltaSource.scala:461)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSourceBase.getFileChangesWithRateLimit(DeltaSource.scala:183)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSourceBase.getFileChangesWithRateLimit$(DeltaSource.scala:172)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.getFileChangesWithRateLimit(DeltaSource.scala:461)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSourceBase.getNextOffsetFromPreviousOffset(DeltaSource.scala:292)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSourceBase.getNextOffsetFromPreviousOffset$(DeltaSource.scala:285)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.getNextOffsetFromPreviousOffset(DeltaSource.scala:461)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.latestOffsetInternal(DeltaSource.scala:657)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.latestOffset(DeltaSource.scala:648)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:449)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:448)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:437)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:693)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:433)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('CREATE DATABASE IF NOT EXISTS etl_tiers')\n",
    "spark.sql('DROP TABLE IF EXISTS etl_tiers.raw')\n",
    "spark.sql('DROP TABLE IF EXISTS etl_tiers.bronze')\n",
    "spark.sql('DROP TABLE IF EXISTS etl_tiers.silver')\n",
    "spark.sql('DROP TABLE IF EXISTS etl_tiers.gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c30753",
   "metadata": {},
   "source": [
    "## Create raw table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8765cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/22 18:26:45 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `etl_tiers`.`raw` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  value\n",
       "0    3    1.0\n",
       "1    1    1.0\n",
       "2    2    1.0\n",
       "3    4    1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1,1.0], [2,1.0], [3,1.0], [4,1.0]]\n",
    "schema = StructType([\n",
    "    StructField('idx', LongType(), False),\n",
    "    StructField('value', DoubleType(), False),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame([], schema=schema)\n",
    "df.write.partitionBy('idx').format('delta').mode('overwrite').saveAsTable('etl_tiers.raw')\n",
    "spark.sql(\"ALTER TABLE etl_tiers.raw SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.write.format('delta').mode('append').saveAsTable('etl_tiers.raw')\n",
    "\n",
    "spark.sql(\"SELECT * FROM etl_tiers.raw\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71c222de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "      <th>engineInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:46.091</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Append', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>True</td>\n",
       "      <td>{'numOutputRows': '4', 'numOutputBytes': '1956...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-22 18:26:45.497</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SET TBLPROPERTIES</td>\n",
       "      <td>{'properties': '{\"delta.enableChangeDataFeed\":...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>True</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-02-22 18:26:45.024</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CREATE OR REPLACE TABLE AS SELECT</td>\n",
       "      <td>{'description': None, 'partitionBy': '[\"idx\"]'...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '0', 'numOutputBytes': '0', ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName  \\\n",
       "0        2 2023-02-22 18:26:46.091   None     None   \n",
       "1        1 2023-02-22 18:26:45.497   None     None   \n",
       "2        0 2023-02-22 18:26:45.024   None     None   \n",
       "\n",
       "                           operation  \\\n",
       "0                              WRITE   \n",
       "1                  SET TBLPROPERTIES   \n",
       "2  CREATE OR REPLACE TABLE AS SELECT   \n",
       "\n",
       "                                 operationParameters   job notebook clusterId  \\\n",
       "0            {'mode': 'Append', 'partitionBy': '[]'}  None     None      None   \n",
       "1  {'properties': '{\"delta.enableChangeDataFeed\":...  None     None      None   \n",
       "2  {'description': None, 'partitionBy': '[\"idx\"]'...  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend  \\\n",
       "0          1.0   Serializable           True   \n",
       "1          0.0   Serializable           True   \n",
       "2          NaN   Serializable          False   \n",
       "\n",
       "                                    operationMetrics userMetadata  \\\n",
       "0  {'numOutputRows': '4', 'numOutputBytes': '1956...         None   \n",
       "1                                                 {}         None   \n",
       "2  {'numOutputRows': '0', 'numOutputBytes': '0', ...         None   \n",
       "\n",
       "                            engineInfo  \n",
       "0  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "1  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "2  Apache-Spark/3.3.1 Delta-Lake/2.2.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describes table history\n",
    "spark.sql('describe history etl_tiers.raw').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32d57d",
   "metadata": {},
   "source": [
    "## Copy inserted data to bronze table using streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "280616c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/22 18:26:46 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-679c152f-1716-4c86-9376-7915a4e316c2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/02/22 18:26:46 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/02/22 18:26:46 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/02/22 18:26:46 ERROR MicroBatchExecution: Query [id = 451f0320-dc82-46a7-8179-cd9684d7664f, runId = 22e53847-5b76-410c-926d-1fa94745b072] terminated with error\n",
      "org.apache.spark.sql.delta.DeltaIllegalStateException: Delta table 11e9442d-85d6-459f-81ef-e8958030b8cb doesn't exist. Please delete your streaming query checkpoint and restart.\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.nonExistentDeltaTableStreaming(DeltaErrors.scala:1374)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.nonExistentDeltaTableStreaming$(DeltaErrors.scala:1373)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrors$.nonExistentDeltaTableStreaming(DeltaErrors.scala:2489)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSourceOffset$.apply(DeltaSourceOffset.scala:97)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.getBatch(DeltaSource.scala:799)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$populateStartOffsets$4(MicroBatchExecution.scala:354)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamProgress.foreach(StreamProgress.scala:27)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.populateStartOffsets(MicroBatchExecution.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:224)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "23/02/22 18:26:47 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `etl_tiers`.`bronze` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>value</th>\n",
       "      <th>_change_type</th>\n",
       "      <th>_commit_version</th>\n",
       "      <th>_commit_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:46.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:46.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:46.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:46.091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  value _change_type  _commit_version       _commit_timestamp\n",
       "0    4    1.0       insert                2 2023-02-22 18:26:46.091\n",
       "1    2    1.0       insert                2 2023-02-22 18:26:46.091\n",
       "2    3    1.0       insert                2 2023-02-22 18:26:46.091\n",
       "3    1    1.0       insert                2 2023-02-22 18:26:46.091"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "      <th>engineInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-02-22 18:26:47.161</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CREATE OR REPLACE TABLE AS SELECT</td>\n",
       "      <td>{'description': None, 'partitionBy': '[]', 'pr...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '4', 'numOutputBytes': '6400...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName  \\\n",
       "0        0 2023-02-22 18:26:47.161   None     None   \n",
       "\n",
       "                           operation  \\\n",
       "0  CREATE OR REPLACE TABLE AS SELECT   \n",
       "\n",
       "                                 operationParameters   job notebook clusterId  \\\n",
       "0  {'description': None, 'partitionBy': '[]', 'pr...  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend  \\\n",
       "0          NaN   Serializable          False   \n",
       "\n",
       "                                    operationMetrics userMetadata  \\\n",
       "0  {'numOutputRows': '4', 'numOutputBytes': '6400...         None   \n",
       "\n",
       "                            engineInfo  \n",
       "0  Apache-Spark/3.3.1 Delta-Lake/2.2.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>value</th>\n",
       "      <th>_change_type</th>\n",
       "      <th>_commit_version</th>\n",
       "      <th>_commit_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3.14</td>\n",
       "      <td>update_preimage</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-22 02:12:41.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>3.14</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-22 02:12:41.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.14</td>\n",
       "      <td>update_preimage</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-22 02:12:41.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.14</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-22 02:12:41.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>update_preimage</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-22 02:05:59.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>3.14</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-22 02:05:59.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>update_preimage</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-22 02:05:59.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>3.14</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-22 02:05:59.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-22 01:47:47.709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>delete</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 01:47:45.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 01:50:30.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-22 01:47:47.709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-22 01:47:47.709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 01:50:30.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-22 01:47:47.709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>delete</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 01:47:45.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>delete</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 01:47:45.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 01:50:30.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>delete</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 01:47:45.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 01:50:30.010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx  value      _change_type  _commit_version       _commit_timestamp\n",
       "0     4   3.14   update_preimage                4 2023-02-22 02:12:41.062\n",
       "1     4   3.14  update_postimage                4 2023-02-22 02:12:41.062\n",
       "2     3   3.14   update_preimage                4 2023-02-22 02:12:41.062\n",
       "3     3   3.14  update_postimage                4 2023-02-22 02:12:41.062\n",
       "4     3   1.00   update_preimage                3 2023-02-22 02:05:59.913\n",
       "5     3   3.14  update_postimage                3 2023-02-22 02:05:59.913\n",
       "6     4   1.00   update_preimage                3 2023-02-22 02:05:59.913\n",
       "7     4   3.14  update_postimage                3 2023-02-22 02:05:59.913\n",
       "8     2   1.00            insert                4 2023-02-22 01:47:47.709\n",
       "9     2   1.00            delete                2 2023-02-22 01:47:45.467\n",
       "10    2   1.00            insert                2 2023-02-22 01:50:30.010\n",
       "11    1   1.00            insert                4 2023-02-22 01:47:47.709\n",
       "12    3   1.00            insert                4 2023-02-22 01:47:47.709\n",
       "13    3   1.00            insert                2 2023-02-22 01:50:30.010\n",
       "14    4   1.00            insert                4 2023-02-22 01:47:47.709\n",
       "15    3   1.00            delete                2 2023-02-22 01:47:45.467\n",
       "16    1   1.00            delete                2 2023-02-22 01:47:45.467\n",
       "17    1   1.00            insert                2 2023-02-22 01:50:30.010\n",
       "18    4   1.00            delete                2 2023-02-22 01:47:45.467\n",
       "19    4   1.00            insert                2 2023-02-22 01:50:30.010"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_table = 'etl_tiers.raw'\n",
    "target_table = 'etl_tiers.bronze'\n",
    "\n",
    "b_etl = BronzeETL(spark)\n",
    "b_etl.etl(source_table, '', target_table)\n",
    "\n",
    "from time import sleep\n",
    "sleep(10)\n",
    "\n",
    "display(spark.sql('SELECT * FROM etl_tiers.bronze').toPandas())\n",
    "display(spark.sql('DESCRIBE HISTORY etl_tiers.bronze').toPandas())\n",
    "\n",
    "spark.sql('use database default')\n",
    "spark.sql('select * from stream').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cf1fcda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/22 18:26:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-41f4e09a-1bff-413f-8410-d96d39a836e8. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/02/22 18:26:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/02/22 18:26:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/02/22 18:26:57 ERROR MicroBatchExecution: Query [id = 451f0320-dc82-46a7-8179-cd9684d7664f, runId = ceaeb6b8-1570-4b9d-adca-14e4296f26ce] terminated with error\n",
      "org.apache.spark.sql.delta.DeltaIllegalStateException: Delta table 11e9442d-85d6-459f-81ef-e8958030b8cb doesn't exist. Please delete your streaming query checkpoint and restart.\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.nonExistentDeltaTableStreaming(DeltaErrors.scala:1374)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.nonExistentDeltaTableStreaming$(DeltaErrors.scala:1373)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrors$.nonExistentDeltaTableStreaming(DeltaErrors.scala:2489)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSourceOffset$.apply(DeltaSourceOffset.scala:97)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.getBatch(DeltaSource.scala:799)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$populateStartOffsets$4(MicroBatchExecution.scala:354)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamProgress.foreach(StreamProgress.scala:27)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.populateStartOffsets(MicroBatchExecution.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:224)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>value</th>\n",
       "      <th>_change_type</th>\n",
       "      <th>_commit_version</th>\n",
       "      <th>_commit_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:46.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:46.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:46.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:46.091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  value _change_type  _commit_version       _commit_timestamp\n",
       "0    1    1.0       insert                2 2023-02-22 18:26:46.091\n",
       "1    2    1.0       insert                2 2023-02-22 18:26:46.091\n",
       "2    3    1.0       insert                2 2023-02-22 18:26:46.091\n",
       "3    4    1.0       insert                2 2023-02-22 18:26:46.091"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "      <th>engineInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-22 18:26:58.540</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MERGE</td>\n",
       "      <td>{'matchedPredicates': '[{\"actionType\":\"update\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '4', 'numTargetRowsInserted'...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-02-22 18:26:47.161</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CREATE OR REPLACE TABLE AS SELECT</td>\n",
       "      <td>{'description': None, 'partitionBy': '[]', 'pr...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '4', 'numOutputBytes': '6400...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName  \\\n",
       "0        1 2023-02-22 18:26:58.540   None     None   \n",
       "1        0 2023-02-22 18:26:47.161   None     None   \n",
       "\n",
       "                           operation  \\\n",
       "0                              MERGE   \n",
       "1  CREATE OR REPLACE TABLE AS SELECT   \n",
       "\n",
       "                                 operationParameters   job notebook clusterId  \\\n",
       "0  {'matchedPredicates': '[{\"actionType\":\"update\"...  None     None      None   \n",
       "1  {'description': None, 'partitionBy': '[]', 'pr...  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend  \\\n",
       "0          0.0   Serializable          False   \n",
       "1          NaN   Serializable          False   \n",
       "\n",
       "                                    operationMetrics userMetadata  \\\n",
       "0  {'numOutputRows': '4', 'numTargetRowsInserted'...         None   \n",
       "1  {'numOutputRows': '4', 'numOutputBytes': '6400...         None   \n",
       "\n",
       "                            engineInfo  \n",
       "0  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "1  Apache-Spark/3.3.1 Delta-Lake/2.2.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "      <th>engineInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-22 18:26:58.540</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MERGE</td>\n",
       "      <td>{'matchedPredicates': '[{\"actionType\":\"update\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '4', 'numTargetRowsInserted'...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-02-22 18:26:47.161</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CREATE OR REPLACE TABLE AS SELECT</td>\n",
       "      <td>{'description': None, 'partitionBy': '[]', 'pr...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '4', 'numOutputBytes': '6400...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName  \\\n",
       "0        1 2023-02-22 18:26:58.540   None     None   \n",
       "1        0 2023-02-22 18:26:47.161   None     None   \n",
       "\n",
       "                           operation  \\\n",
       "0                              MERGE   \n",
       "1  CREATE OR REPLACE TABLE AS SELECT   \n",
       "\n",
       "                                 operationParameters   job notebook clusterId  \\\n",
       "0  {'matchedPredicates': '[{\"actionType\":\"update\"...  None     None      None   \n",
       "1  {'description': None, 'partitionBy': '[]', 'pr...  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend  \\\n",
       "0          0.0   Serializable          False   \n",
       "1          NaN   Serializable          False   \n",
       "\n",
       "                                    operationMetrics userMetadata  \\\n",
       "0  {'numOutputRows': '4', 'numTargetRowsInserted'...         None   \n",
       "1  {'numOutputRows': '4', 'numOutputBytes': '6400...         None   \n",
       "\n",
       "                            engineInfo  \n",
       "0  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "1  Apache-Spark/3.3.1 Delta-Lake/2.2.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/22 18:27:00 ERROR MicroBatchExecution: Query [id = 5aa0cb13-c572-4846-b81b-918b83985aae, runId = dd1700e2-be7e-4a73-8233-bad014212745] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 272, in call\n",
      "    raise e\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 269, in call\n",
      "    self.func(DataFrame(jdf, self.session), batch_id)\n",
      "  File \"<string>\", line 38, in merge\n",
      "  File \"<string>\", line 27, in merge\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/delta/tables.py\", line 938, in execute\n",
      "    self._jbuilder.execute()\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 196, in deco\n",
      "    raise converted from None\n",
      "delta.exceptions.ConcurrentAppendException: Files were added to the root of the table by a concurrent update. Please try the operation again.\n",
      "Conflicting commit: {\"timestamp\":1677101219613,\"operation\":\"MERGE\",\"operationParameters\":{\"predicate\":(t.idx = s.idx),\"matchedPredicates\":[{\"actionType\":\"update\"}],\"notMatchedPredicates\":[{\"actionType\":\"insert\"}]},\"readVersion\":1,\"isolationLevel\":\"Serializable\",\"isBlindAppend\":false,\"operationMetrics\":{\"numTargetRowsCopied\":\"2\",\"numTargetRowsDeleted\":\"0\",\"numTargetFilesAdded\":\"1\",\"executionTimeMs\":\"503\",\"numTargetRowsInserted\":\"0\",\"scanTimeMs\":\"327\",\"numTargetRowsUpdated\":\"2\",\"numOutputRows\":\"4\",\"numTargetChangeFilesAdded\":\"0\",\"numSourceRows\":\"2\",\"numTargetFilesRemoved\":\"1\",\"rewriteTimeMs\":\"166\"},\"engineInfo\":\"Apache-Spark/3.3.1 Delta-Lake/2.2.0\",\"txnId\":\"27e1fdf0-00a7-4337-9c4e-672d0939e227\"}\n",
      "Refer to https://docs.delta.io/latest/concurrency-control.html for more details.\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat com.sun.proxy.$Proxy45.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>value</th>\n",
       "      <th>_change_type</th>\n",
       "      <th>_commit_version</th>\n",
       "      <th>_commit_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:46.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:46.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.14</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-22 18:26:57.964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3.14</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-22 18:26:57.964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  value      _change_type  _commit_version       _commit_timestamp\n",
       "0    1   1.00            insert                2 2023-02-22 18:26:46.091\n",
       "1    2   1.00            insert                2 2023-02-22 18:26:46.091\n",
       "2    3   3.14  update_postimage                3 2023-02-22 18:26:57.964\n",
       "3    4   3.14  update_postimage                3 2023-02-22 18:26:57.964"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_etl = BronzeETL(spark)\n",
    "b_etl.etl(source_table, '', target_table)\n",
    "\n",
    "spark.sql('update etl_tiers.raw set value = 3.14 where idx > 2')\n",
    "display(spark.sql('SELECT * FROM etl_tiers.bronze').toPandas())\n",
    "display(spark.sql('DESCRIBE HISTORY etl_tiers.bronze').toPandas().head(4))\n",
    "\n",
    "display(spark.sql('DESCRIBE HISTORY etl_tiers.bronze').toPandas().head(4))\n",
    "spark.sql('select * from etl_tiers.bronze').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7aa25",
   "metadata": {},
   "source": [
    "## Create table raw2 and insert values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae17dfa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>value</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>asdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>rewq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  value description\n",
       "0    5    1.0        asdf\n",
       "1    6    1.0        rewq"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create raw2 table and insert values\n",
    "\n",
    "data = [[5,1.0, 'asdf'], [6,1.0, 'rewq']]\n",
    "schema = StructType([\n",
    "    StructField('idx', LongType(), False),\n",
    "    StructField('value', DoubleType(), False),\n",
    "    StructField('description', StringType(), False),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame([], schema=schema)\n",
    "df.write.partitionBy('idx').format('delta').mode('overwrite').saveAsTable('etl_tiers.raw2')\n",
    "spark.sql(\"ALTER TABLE etl_tiers.raw2 SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.write.format('delta').mode('append').saveAsTable('etl_tiers.raw2')\n",
    "\n",
    "spark.sql(\"SELECT * FROM etl_tiers.raw2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efc0fa3",
   "metadata": {},
   "source": [
    "## Call bronze ETL again using raw2 as source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "460e14a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/22 18:28:51 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c2b2edb9-6bf5-4001-9304-2c6be4c01113. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/02/22 18:28:51 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/02/22 18:28:51 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/02/22 18:28:51 ERROR MicroBatchExecution: Query [id = 451f0320-dc82-46a7-8179-cd9684d7664f, runId = 8662c4a2-81bf-422b-9950-beb9c4523ba4] terminated with error\n",
      "org.apache.spark.sql.delta.DeltaIllegalStateException: Delta table 11e9442d-85d6-459f-81ef-e8958030b8cb doesn't exist. Please delete your streaming query checkpoint and restart.\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.nonExistentDeltaTableStreaming(DeltaErrors.scala:1374)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.nonExistentDeltaTableStreaming$(DeltaErrors.scala:1373)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrors$.nonExistentDeltaTableStreaming(DeltaErrors.scala:2489)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSourceOffset$.apply(DeltaSourceOffset.scala:97)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.getBatch(DeltaSource.scala:799)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$populateStartOffsets$4(MicroBatchExecution.scala:354)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamProgress.foreach(StreamProgress.scala:27)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.populateStartOffsets(MicroBatchExecution.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:224)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>value</th>\n",
       "      <th>_change_type</th>\n",
       "      <th>_commit_version</th>\n",
       "      <th>_commit_timestamp</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-02-22 18:28:24.889</td>\n",
       "      <td>asdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-02-22 18:28:24.889</td>\n",
       "      <td>rewq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:46.091</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:46.091</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3.14</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-22 18:26:57.964</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>3.14</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-22 18:26:57.964</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  value      _change_type  _commit_version       _commit_timestamp  \\\n",
       "0    5   1.00            insert                8 2023-02-22 18:28:24.889   \n",
       "1    6   1.00            insert                8 2023-02-22 18:28:24.889   \n",
       "2    1   1.00            insert                2 2023-02-22 18:26:46.091   \n",
       "3    2   1.00            insert                2 2023-02-22 18:26:46.091   \n",
       "4    3   3.14  update_postimage                3 2023-02-22 18:26:57.964   \n",
       "5    4   3.14  update_postimage                3 2023-02-22 18:26:57.964   \n",
       "\n",
       "  description  \n",
       "0        asdf  \n",
       "1        rewq  \n",
       "2        None  \n",
       "3        None  \n",
       "4        None  \n",
       "5        None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "      <th>engineInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-22 18:28:26.485</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MERGE</td>\n",
       "      <td>{'matchedPredicates': '[{\"actionType\":\"update\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '2', 'numTargetRowsInserted'...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-22 18:28:24.955</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MERGE</td>\n",
       "      <td>{'matchedPredicates': '[{\"actionType\":\"update\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '0', 'numTargetRowsInserted'...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:59.616</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MERGE</td>\n",
       "      <td>{'matchedPredicates': '[{\"actionType\":\"update\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '4', 'numTargetRowsInserted'...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-22 18:26:58.540</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MERGE</td>\n",
       "      <td>{'matchedPredicates': '[{\"actionType\":\"update\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '4', 'numTargetRowsInserted'...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName operation  \\\n",
       "0        4 2023-02-22 18:28:26.485   None     None     MERGE   \n",
       "1        3 2023-02-22 18:28:24.955   None     None     MERGE   \n",
       "2        2 2023-02-22 18:26:59.616   None     None     MERGE   \n",
       "3        1 2023-02-22 18:26:58.540   None     None     MERGE   \n",
       "\n",
       "                                 operationParameters   job notebook clusterId  \\\n",
       "0  {'matchedPredicates': '[{\"actionType\":\"update\"...  None     None      None   \n",
       "1  {'matchedPredicates': '[{\"actionType\":\"update\"...  None     None      None   \n",
       "2  {'matchedPredicates': '[{\"actionType\":\"update\"...  None     None      None   \n",
       "3  {'matchedPredicates': '[{\"actionType\":\"update\"...  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend  \\\n",
       "0          3.0   Serializable          False   \n",
       "1          2.0   Serializable          False   \n",
       "2          1.0   Serializable          False   \n",
       "3          0.0   Serializable          False   \n",
       "\n",
       "                                    operationMetrics userMetadata  \\\n",
       "0  {'numOutputRows': '2', 'numTargetRowsInserted'...         None   \n",
       "1  {'numOutputRows': '0', 'numTargetRowsInserted'...         None   \n",
       "2  {'numOutputRows': '4', 'numTargetRowsInserted'...         None   \n",
       "3  {'numOutputRows': '4', 'numTargetRowsInserted'...         None   \n",
       "\n",
       "                            engineInfo  \n",
       "0  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "1  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "2  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "3  Apache-Spark/3.3.1 Delta-Lake/2.2.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>value</th>\n",
       "      <th>_change_type</th>\n",
       "      <th>_commit_version</th>\n",
       "      <th>_commit_timestamp</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-02-22 18:28:24.889</td>\n",
       "      <td>asdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-02-22 18:28:24.889</td>\n",
       "      <td>rewq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:46.091</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:46.091</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3.14</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-22 18:26:57.964</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>3.14</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-22 18:26:57.964</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  value      _change_type  _commit_version       _commit_timestamp  \\\n",
       "0    5   1.00            insert                8 2023-02-22 18:28:24.889   \n",
       "1    6   1.00            insert                8 2023-02-22 18:28:24.889   \n",
       "2    1   1.00            insert                2 2023-02-22 18:26:46.091   \n",
       "3    2   1.00            insert                2 2023-02-22 18:26:46.091   \n",
       "4    3   3.14  update_postimage                3 2023-02-22 18:26:57.964   \n",
       "5    4   3.14  update_postimage                3 2023-02-22 18:26:57.964   \n",
       "\n",
       "  description  \n",
       "0        asdf  \n",
       "1        rewq  \n",
       "2        None  \n",
       "3        None  \n",
       "4        None  \n",
       "5        None  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_etl = BronzeETL(spark)\n",
    "b_etl.etl('etl_tiers.raw2', '', target_table)\n",
    "\n",
    "display(spark.sql('SELECT * FROM etl_tiers.bronze').toPandas())\n",
    "display(spark.sql('DESCRIBE HISTORY etl_tiers.bronze').toPandas().head(4))\n",
    "spark.sql('select * from etl_tiers.bronze').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ad7ee0",
   "metadata": {},
   "source": [
    "## Check number of rows updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e68bd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'numOutputRows': '2',\n",
       " 'numTargetRowsInserted': '0',\n",
       " 'numTargetRowsUpdated': '2',\n",
       " 'numTargetFilesAdded': '1',\n",
       " 'numTargetFilesRemoved': '1',\n",
       " 'numTargetRowsDeleted': '0',\n",
       " 'scanTimeMs': '221',\n",
       " 'numSourceRows': '2',\n",
       " 'numTargetChangeFilesAdded': '0',\n",
       " 'executionTimeMs': '411',\n",
       " 'numTargetRowsCopied': '0',\n",
       " 'rewriteTimeMs': '181'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 56042)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE HISTORY etl_tiers.bronze').toPandas().loc[0, 'operationMetrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa258e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

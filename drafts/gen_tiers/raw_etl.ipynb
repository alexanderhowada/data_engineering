{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fabd20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5e875f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_exists(table):\n",
    "    table = table.split('.')\n",
    "    database, table = table[0], table[1]\n",
    "    \n",
    "    if spark._jsparkSession.catalog().tableExists(database, table):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def merge(df, target_table, pk, spark_session=None, partition=[]):\n",
    "    \n",
    "    if spark_session is None:\n",
    "        global spark\n",
    "    else:\n",
    "        spark = spark_session\n",
    "        \n",
    "    if table_exists(target_table):\n",
    "        dt = DeltaTable.forName(spark, target_table)\n",
    "        \n",
    "        condition = [f't.{k}=s.{k}' for k in pk]\n",
    "        condition = ' and '.join(condition)\n",
    "        \n",
    "        dt.alias('t').merge(\n",
    "            df.alias('s'), condition\n",
    "        ).whenMatchedUpdateAll() \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()\n",
    "    else:\n",
    "        if len(partition) > 0:\n",
    "            df.write.partitionBy(*partition).mode('overwrite').format('delta').saveAsTable(target_table)\n",
    "        else:\n",
    "            df.write.mode('overwrite').format('delta').saveAsTable(target_table)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaf6786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawETL:\n",
    "    \n",
    "    pk = ['idx']\n",
    "    \n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "    \n",
    "    def extract(self, source_table):\n",
    "\n",
    "        df = self.spark.read.csv(source_table, header=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \n",
    "        for c in df.columns:\n",
    "            df = df.withColumn(c, df[c].cast(StringType()))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def load(self, df, target_table):\n",
    "        \n",
    "        merge(df, target_table, self.pk, self.spark, partition=self.pk)\n",
    "        \n",
    "    def etl(self, target_table, source_table, e_kwargs={}, t_kwargs={}, l_kwargs={}):\n",
    "        \n",
    "        df = self.extract(source_table, **e_kwargs)\n",
    "        df = self.transform(df, **t_kwargs)\n",
    "        self.load(df, target_table, **l_kwargs)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b4b3e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: delta.enableChangeDataFeed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/10 04:47:07 WARN Utils: Your hostname, spiriel resolves to a loopback address: 127.0.1.1; using 192.168.15.6 instead (on interface enp3s0)\n",
      "23/02/10 04:47:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/ahow/main_env/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ahow/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ahow/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f5b6adaa-e9d2-4bcf-98af-722432baa6e0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.2.0 in central\n",
      "\tfound io.delta#delta-storage;2.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 90ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f5b6adaa-e9d2-4bcf-98af-722432baa6e0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/10 04:47:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/10 04:47:15 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:===============================>                        (28 + 12) / 50]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/10 04:47:17 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `dummy`.`raw` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|    dummy|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>value</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>asdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>asdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>asdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>asdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>asdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>asdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>asdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>asdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>asdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  idx value  type\n",
       "0  11    15  asdf\n",
       "1  14    10  asdf\n",
       "2   1    15  asdf\n",
       "3  12    10  asdf\n",
       "4   4    15  asdf\n",
       "5   3    15  asdf\n",
       "6  13    18  asdf\n",
       "7   2    15  asdf\n",
       "8   5    15  asdf"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    from pyspark.sql import SparkSession\n",
    "    from delta.pip_utils import configure_spark_with_delta_pip\n",
    "    \n",
    "    builder = SparkSession.builder\\\n",
    "           .appName('raw_etl')\\\n",
    "           .config('spark.sql.warehouse.dir', 'pyspark_tables')\\\n",
    "           .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "           .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "           .config('spark.databricks.delta.retentionDurationCheck.enabled', False) \\\n",
    "           .config('spark.databricks.delta.schema.autoMerge.enabled', True) \\\n",
    "           .config('delta.enableChangeDataFeed', True)\n",
    "\n",
    "    spark = configure_spark_with_delta_pip(builder).enableHiveSupport().getOrCreate()\n",
    "    \n",
    "    TARGET_TABLE = 'dummy.raw'\n",
    "    SOURCE_TABLE = 'raw.csv'\n",
    "    \n",
    "    etl = rawEtl(spark)\n",
    "    etl.etl(TARGET_TABLE, SOURCE_TABLE)\n",
    "    \n",
    "    spark.sql('SHOW DATABASES').show()\n",
    "    \n",
    "    df = spark.sql('SELECT * FROM dummy.raw').toPandas()\n",
    "    \n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abca7dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

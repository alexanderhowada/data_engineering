{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "579f4ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.pip_utils import configure_spark_with_delta_pip\n",
    "\n",
    "from utils.string_utils import camel_to_snake, normalize_string\n",
    "from engine import SupermetricsAPI\n",
    "from etl import default, get_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b30e3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sm_normalize_cols(s):\n",
    "    s = normalize_string(s)\n",
    "    s = camel_to_snake(s)\n",
    "    s = s.replace(' ', '_')\n",
    "    s = re.sub(r'_+', '_', s)\n",
    "    s = s.strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ff63af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: delta.enableChangeDataFeed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/18 01:56:54 WARN Utils: Your hostname, spiriel resolves to a loopback address: 127.0.1.1; using 192.168.15.6 instead (on interface enp3s0)\n",
      "23/04/18 01:56:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/ahow/main_env/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ahow/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ahow/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a82864aa-7cdf-42e9-8a23-5b7f25806715;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.2.0 in central\n",
      "\tfound io.delta#delta-storage;2.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 89ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a82864aa-7cdf-42e9-8a23-5b7f25806715\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/18 01:56:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = SparkSession.builder \\\n",
    "    .appName('test_delta_utils') \\\n",
    "    .config('spark.sql.warehouse.dir', 'pyspark_tables') \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config('spark.databricks.delta.retentionDurationCheck.enabled', False) \\\n",
    "    .config('spark.databricks.delta.schema.autoMerge.enabled', True) \\\n",
    "    .config('spark.databricks.delta.checkLatestSchemaOnRead', True) \\\n",
    "    .config(\"spark.log.level\", \"ERROR\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .config('delta.enableChangeDataFeed', True)\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).enableHiveSupport().getOrCreate()\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS supermetrics\")\n",
    "\n",
    "tb = 'supermetrics.test1'\n",
    "url = os.environ['URL']\n",
    "pk = ['Sheet', 'Column A']\n",
    "partition = ['Sheet']\n",
    "dt_start = datetime(2022, 1, 1)\n",
    "dt_end = datetime(2022, 3, 1)\n",
    "dt_delta = timedelta(days=15)\n",
    "sm_kwargs = {'offset': 100000}\n",
    "r_kwargs = {'timeout': 600}\n",
    "\n",
    "api = SupermetricsAPI(url, **sm_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "619339f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "*********** Starting SM ETL **********\n",
      "**************************************\n",
      "from 01/01/2022 to 15/01/2022\n",
      "extracting\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'paginate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = get_df(api, spark, dt_start, dt_end, r_kwargs=r_kwargs, replace_f=[sm_normalize_cols])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# df.show()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdt_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_delta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mr_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mr_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msm_normalize_cols\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MyGitHub/data_engineering/supermetrics/etl.py:91\u001b[0m, in \u001b[0;36mdefault\u001b[0;34m(api, spark_session, tb, pk, partition, dt_start, dt_end, dt_delta, subtract_ed, r_kwargs, replace_f)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdt_start\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdt_end\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextracting\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdt_start\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdt_end\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mr_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m df \u001b[38;5;241m=\u001b[39m pandas_to_spark_string(df, spark_session\u001b[38;5;241m=\u001b[39mspark_session, col_f\u001b[38;5;241m=\u001b[39mreplace_f)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerging\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/MyGitHub/data_engineering/supermetrics/engine.py:106\u001b[0m, in \u001b[0;36mSupermetricsAPI.extract_data\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 106\u001b[0m     j_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(j_list[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n",
      "File \u001b[0;32m~/MyGitHub/data_engineering/supermetrics/engine.py:99\u001b[0m, in \u001b[0;36mSupermetricsAPI.extract_json\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 99\u001b[0m     r_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     j_list \u001b[38;5;241m=\u001b[39m [r\u001b[38;5;241m.\u001b[39mjson() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m r_list]\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m j_list\n",
      "File \u001b[0;32m~/MyGitHub/data_engineering/supermetrics/engine.py:90\u001b[0m, in \u001b[0;36mSupermetricsAPI.extract\u001b[0;34m(self, dt_start, dt_end, r_kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_request(url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mr_kwargs)\n\u001b[1;32m     88\u001b[0m r_list \u001b[38;5;241m=\u001b[39m [r]\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmeta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpaginate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     url \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaginate\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     92\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_request(url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mr_kwargs)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'paginate'"
     ]
    }
   ],
   "source": [
    "# df = get_df(api, spark, dt_start, dt_end, r_kwargs=r_kwargs, replace_f=[sm_normalize_cols])\n",
    "# df.show()\n",
    "\n",
    "default(\n",
    "    api, spark, tb, pk, partition,\n",
    "    dt_start, dt_end, dt_delta,\n",
    "    r_kwargs=r_kwargs, replace_f=[sm_normalize_cols]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "073b3d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"meta\":{\"request_id\":\"WSgBVhSAzjXB56vAe4xo7Ron8BbyKaaK\"},\"error\":{\"code\":\"LICENSE_DAILY_QUERY_LIMIT_EXCEEDED\",\"message\":\"LICENSE_DAILY_QUERY_LIMIT_EXCEEDED\",\"description\":\"Your license limit of 100 queries per day has been exceeded for Google Sheets.\"}}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.last_request.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dfd7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e7615cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Utils/delta_utils.ipynb import merge\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca68b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(path, data):\n",
    "    data = json.dumps(data, indent=4)\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c87baf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawETLJSON:\n",
    "    \n",
    "    pk = ['idx']\n",
    "    \n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "        \n",
    "    def extract(self, folder):\n",
    "        stream = self.spark.readStream.json(folder, primitivesAsString=True, multiLine=True)\n",
    "    \n",
    "        return stream\n",
    "    \n",
    "    def merge(self, batch_df, idx):\n",
    "        merge(batch_df, self.target_table, self.pk, spark_session=self.spark, partition=self.pk)\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df\n",
    "    \n",
    "    def load(self, stream, target_table):\n",
    "        self.target_table = target_table\n",
    "        \n",
    "        query = stream.writeStream.outputMode('update') \\\n",
    "                .option(\"checkpointLocation\", f\"./checkpoints/{target_table}\") \\\n",
    "                .foreachBatch(self.merge).start()\n",
    "        return    \n",
    "    \n",
    "    def etl(self, target_table, folder):\n",
    "        stream = self.extract(folder)\n",
    "        self.load(stream, target_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f94cddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BronzeETL:\n",
    "    \n",
    "    pk = ['idx']\n",
    "    \n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "        \n",
    "    def extract(self, target_table, source_table):\n",
    "        \n",
    "        last_ts = self.spark.sql(f'DESCRIBE HISTORY {target_table}').toPandas()\n",
    "        if last_ts.index.size == 0:\n",
    "            last_ts = '1990-01-01 00:00:00'\n",
    "        else:\n",
    "            last_ts = last_ts.loc[0, 'timestamp']\n",
    "        \n",
    "        stream = self.spark.readStream.format('delta') \\\n",
    "                     .option('delta.enableChangeDataFeed', True) \\\n",
    "                     .option('readChangeFeed', True) \\\n",
    "                     .option('startingTimestamp', last_ts) \\\n",
    "                     .table(source_table)\n",
    "        #                      \\\n",
    "    \n",
    "        return stream\n",
    "    \n",
    "    def merge(self, batch_df, idx):\n",
    "        batch_df.show()\n",
    "        batch_df = batch_df.filter(\"_change_type='update_postimage'\")\n",
    "        merge(batch_df, self.target_table, self.pk, spark_session=self.spark, partition=self.pk)\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df\n",
    "    \n",
    "    def load(self, stream, target_table, source_table):\n",
    "        self.target_table = target_table\n",
    "        \n",
    "        query = stream.writeStream.format('delta').outputMode('update') \\\n",
    "                .option(\"checkpointLocation\", f\"./checkpoints/{target_table}\") \\\n",
    "                .foreachBatch(self.merge).start()\n",
    "        #                .option('ignoreChanges', True) \\\n",
    "    \n",
    "    def etl(self, target_table, source_table):\n",
    "        stream = self.extract(target_table, source_table)\n",
    "        self.load(stream, target_table, source_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4a3dc653",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'idx': 10, 'value': 6.0},\n",
    "    {'idx': 11, 'value': 6.0},\n",
    "    {'idx': 12, 'value': 6.0},\n",
    "]\n",
    "write_json('data/json8.json', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c2d582a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/23 02:05:50 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "23/02/23 02:05:50 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/02/23 02:05:51 ERROR MicroBatchExecution: Query [id = c0fa3602-bf96-4cba-8b3a-c779fcc54cf9, runId = e8ba1299-7612-4b57-b264-30566eb9d473] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 272, in call\n",
      "    raise e\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 269, in call\n",
      "    self.func(DataFrame(jdf, self.session), batch_id)\n",
      "  File \"/tmp/ipykernel_75816/349814426.py\", line 14, in merge\n",
      "    merge(batch_df, self.target_table, self.pk, spark_session=self.spark, partition=self.pk)\n",
      "  File \"/tmp/ipykernel_75816/1499376899.py\", line 33, in merge\n",
      "    .execute()\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/delta/tables.py\", line 938, in execute\n",
      "    self._jbuilder.execute()\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1076.execute.\n",
      ": org.apache.spark.sql.delta.DeltaUnsupportedOperationException: Cannot perform Merge as multiple source rows matched and attempted to modify the same\n",
      "target row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\n",
      "when multiple source rows match on the same target row, the result may be ambiguous\n",
      "as it is unclear which source row should be used to update or delete the matching\n",
      "target row. You can preprocess the source table to eliminate the possibility of\n",
      "multiple matches. Please refer to\n",
      "https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrors.scala:1108)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.multipleSourceRowMatchingTargetRowInMergeException$(DeltaErrors.scala:1105)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrors$.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrors.scala:2489)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$findTouchedFiles$1(MergeIntoCommand.scala:490)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordMergeOperation(MergeIntoCommand.scala:1004)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.findTouchedFiles(MergeIntoCommand.scala:426)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2(MergeIntoCommand.scala:367)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2$adapted(MergeIntoCommand.scala:340)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:252)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$1(MergeIntoCommand.scala:340)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordFrameProfile(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:132)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordOperation(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:131)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:121)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:109)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordDeltaOperation(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runMerge(MergeIntoCommand.scala:338)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$1(MergeIntoCommand.scala:333)\n",
      "\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:102)\n",
      "\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:90)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runWithMaterializedSourceLostRetries(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.run(MergeIntoCommand.scala:333)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:230)\n",
      "\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:105)\n",
      "\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:91)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:122)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:206)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat com.sun.proxy.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    from pyspark.sql import SparkSession\n",
    "    from delta.pip_utils import configure_spark_with_delta_pip\n",
    "    \n",
    "    builder = SparkSession.builder\\\n",
    "           .appName('raw_etl')\\\n",
    "           .config('spark.sql.warehouse.dir', 'pyspark_tables')\\\n",
    "           .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "           .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "           .config('spark.databricks.delta.retentionDurationCheck.enabled', False) \\\n",
    "           .config('spark.databricks.delta.schema.autoMerge.enabled', True) \\\n",
    "           .config('spark.databricks.delta.checkLatestSchemaOnRead', True) \\\n",
    "           .config('delta.enableChangeDataFeed', True) \\\n",
    "           .config('spark.sql.streaming.schemaInference', True)\n",
    "    \n",
    "    folder = 'data/'\n",
    "    target_table = 'streaming_json.raw'\n",
    "    spark = configure_spark_with_delta_pip(builder).enableHiveSupport().getOrCreate()\n",
    "    \n",
    "    etl = RawETLJSON(spark)\n",
    "    etl.etl(target_table, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f73b2043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/23 02:01:00 ERROR MicroBatchExecution: Query [id = c0fa3602-bf96-4cba-8b3a-c779fcc54cf9, runId = fa72829a-c7d9-4b73-b56b-f1d1d403c01b] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 272, in call\n",
      "    raise e\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 269, in call\n",
      "    self.func(DataFrame(jdf, self.session), batch_id)\n",
      "  File \"/tmp/ipykernel_75816/349814426.py\", line 14, in merge\n",
      "    merge(batch_df, self.target_table, self.pk, spark_session=self.spark, partition=self.pk)\n",
      "  File \"/tmp/ipykernel_75816/1499376899.py\", line 33, in merge\n",
      "    .execute()\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/delta/tables.py\", line 938, in execute\n",
      "    self._jbuilder.execute()\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1011.execute.\n",
      ": org.apache.spark.sql.delta.DeltaUnsupportedOperationException: Cannot perform Merge as multiple source rows matched and attempted to modify the same\n",
      "target row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\n",
      "when multiple source rows match on the same target row, the result may be ambiguous\n",
      "as it is unclear which source row should be used to update or delete the matching\n",
      "target row. You can preprocess the source table to eliminate the possibility of\n",
      "multiple matches. Please refer to\n",
      "https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrors.scala:1108)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.multipleSourceRowMatchingTargetRowInMergeException$(DeltaErrors.scala:1105)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrors$.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrors.scala:2489)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$findTouchedFiles$1(MergeIntoCommand.scala:490)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordMergeOperation(MergeIntoCommand.scala:1004)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.findTouchedFiles(MergeIntoCommand.scala:426)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2(MergeIntoCommand.scala:367)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2$adapted(MergeIntoCommand.scala:340)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:252)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$1(MergeIntoCommand.scala:340)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordFrameProfile(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:132)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordOperation(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:131)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:121)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:109)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordDeltaOperation(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runMerge(MergeIntoCommand.scala:338)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$1(MergeIntoCommand.scala:333)\n",
      "\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:102)\n",
      "\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:90)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runWithMaterializedSourceLostRetries(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.run(MergeIntoCommand.scala:333)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:230)\n",
      "\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:105)\n",
      "\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:91)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:122)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:206)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat com.sun.proxy.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "23/02/23 02:01:00 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/02/23 02:01:00 WARN StreamingQueryManager: Stopping existing streaming query [id=92f85b4e-093c-4fb9-a4a8-a1c633fc4319, runId=80344ae7-5113-4bfd-a20f-a987555f26cb], as a new run is being started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/23 02:01:00 ERROR MicroBatchExecution: Query [id = 92f85b4e-093c-4fb9-a4a8-a1c633fc4319, runId = c96bfef4-a5b7-4da2-8fff-288939f2ab28] terminated with error\n",
      "org.apache.spark.sql.delta.DeltaAnalysisException: The provided timestamp (2023-02-23 01:52:19.912) is after the latest version available to this\n",
      "table (2023-02-22 23:35:10.549). Please use a timestamp before or at 2023-02-22 23:35:10.\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.timestampGreaterThanLatestCommit(DeltaErrors.scala:1302)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.timestampGreaterThanLatestCommit$(DeltaErrors.scala:1298)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrors$.timestampGreaterThanLatestCommit(DeltaErrors.scala:2489)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource$.getStartingVersionFromTimestamp(DeltaSource.scala:1046)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.getStartingVersion$lzycompute(DeltaSource.scala:993)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.getStartingVersion(DeltaSource.scala:966)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaSource.getBatch(DeltaSource.scala:810)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$populateStartOffsets$4(MicroBatchExecution.scala:354)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamProgress.foreach(StreamProgress.scala:27)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.populateStartOffsets(MicroBatchExecution.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:224)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    target_table = 'streaming_json.bronze'\n",
    "    source_table = 'streaming_json.raw'\n",
    "    \n",
    "    etl = BronzeETL(spark)\n",
    "    etl.etl(target_table, source_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ff69c9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>value</th>\n",
       "      <th>_change_type</th>\n",
       "      <th>_commit_version</th>\n",
       "      <th>_commit_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [idx, value, _change_type, _commit_version, _commit_timestamp]\n",
       "Index: []"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM streaming_json.bronze').toPandas()\n",
    "#spark.sql('DELETE FROM streaming_json.bronze').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "41cbdf93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx value\n",
       "0   11   6.0\n",
       "1   12   6.0\n",
       "2   10   6.0\n",
       "3    1   4.0\n",
       "4    4   4.0\n",
       "5    3   4.0\n",
       "6    9   4.0\n",
       "7    2   4.0\n",
       "8    5   1.0\n",
       "9    6   1.0\n",
       "10   8   4.0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM streaming_json.raw').toPandas()\n",
    "#spark.sql('DESCRIBE HISTORY streaming_json.raw').toPandas().loc[2,'operationParameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "78463b23",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: streaming_json.bronze; line 1 pos 14;\n'Sort ['idx ASC NULLS FIRST], true\n+- 'Project [*]\n   +- 'UnresolvedRelation [streaming_json, bronze], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mselect * from streaming_json.bronze order by idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoPandas()\n",
      "File \u001b[0;32m~/main_env/lib/python3.10/site-packages/pyspark/sql/session.py:1034\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     sqlQuery \u001b[38;5;241m=\u001b[39m formatter\u001b[38;5;241m.\u001b[39mformat(sqlQuery, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/main_env/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: streaming_json.bronze; line 1 pos 14;\n'Sort ['idx ASC NULLS FIRST], true\n+- 'Project [*]\n   +- 'UnresolvedRelation [streaming_json, bronze], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from streaming_json.bronze order by idx').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25b234b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "      <th>engineInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>2023-02-22 23:16:42.531</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SET TBLPROPERTIES</td>\n",
       "      <td>{'properties': '{\"delta.enableChangeDataFeed\":...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>True</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>2023-02-22 23:15:44.253</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MERGE</td>\n",
       "      <td>{'matchedPredicates': '[{\"actionType\":\"update\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '3', 'numTargetRowsInserted'...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-22 23:15:07.368</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SET TBLPROPERTIES</td>\n",
       "      <td>{'properties': '{\"delta.enableChangeDataFeed\":...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>True</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-22 20:45:08.060</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MERGE</td>\n",
       "      <td>{'matchedPredicates': '[{\"actionType\":\"update\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '6', 'numTargetRowsInserted'...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-22 20:44:45.590</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MERGE</td>\n",
       "      <td>{'matchedPredicates': '[{\"actionType\":\"update\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '4', 'numTargetRowsInserted'...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 20:40:42.763</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MERGE</td>\n",
       "      <td>{'matchedPredicates': '[{\"actionType\":\"update\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '4', 'numTargetRowsInserted'...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-22 20:40:20.535</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MERGE</td>\n",
       "      <td>{'matchedPredicates': '[{\"actionType\":\"update\"...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '4', 'numTargetRowsInserted'...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-02-22 20:37:43.615</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CREATE OR REPLACE TABLE AS SELECT</td>\n",
       "      <td>{'description': None, 'partitionBy': '[\"idx\"]'...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>False</td>\n",
       "      <td>{'numOutputRows': '4', 'numOutputBytes': '1852...</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.3.1 Delta-Lake/2.2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName  \\\n",
       "0        7 2023-02-22 23:16:42.531   None     None   \n",
       "1        6 2023-02-22 23:15:44.253   None     None   \n",
       "2        5 2023-02-22 23:15:07.368   None     None   \n",
       "3        4 2023-02-22 20:45:08.060   None     None   \n",
       "4        3 2023-02-22 20:44:45.590   None     None   \n",
       "5        2 2023-02-22 20:40:42.763   None     None   \n",
       "6        1 2023-02-22 20:40:20.535   None     None   \n",
       "7        0 2023-02-22 20:37:43.615   None     None   \n",
       "\n",
       "                           operation  \\\n",
       "0                  SET TBLPROPERTIES   \n",
       "1                              MERGE   \n",
       "2                  SET TBLPROPERTIES   \n",
       "3                              MERGE   \n",
       "4                              MERGE   \n",
       "5                              MERGE   \n",
       "6                              MERGE   \n",
       "7  CREATE OR REPLACE TABLE AS SELECT   \n",
       "\n",
       "                                 operationParameters   job notebook clusterId  \\\n",
       "0  {'properties': '{\"delta.enableChangeDataFeed\":...  None     None      None   \n",
       "1  {'matchedPredicates': '[{\"actionType\":\"update\"...  None     None      None   \n",
       "2  {'properties': '{\"delta.enableChangeDataFeed\":...  None     None      None   \n",
       "3  {'matchedPredicates': '[{\"actionType\":\"update\"...  None     None      None   \n",
       "4  {'matchedPredicates': '[{\"actionType\":\"update\"...  None     None      None   \n",
       "5  {'matchedPredicates': '[{\"actionType\":\"update\"...  None     None      None   \n",
       "6  {'matchedPredicates': '[{\"actionType\":\"update\"...  None     None      None   \n",
       "7  {'description': None, 'partitionBy': '[\"idx\"]'...  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend  \\\n",
       "0          6.0   Serializable           True   \n",
       "1          5.0   Serializable          False   \n",
       "2          4.0   Serializable           True   \n",
       "3          3.0   Serializable          False   \n",
       "4          2.0   Serializable          False   \n",
       "5          1.0   Serializable          False   \n",
       "6          0.0   Serializable          False   \n",
       "7          NaN   Serializable          False   \n",
       "\n",
       "                                    operationMetrics userMetadata  \\\n",
       "0                                                 {}         None   \n",
       "1  {'numOutputRows': '3', 'numTargetRowsInserted'...         None   \n",
       "2                                                 {}         None   \n",
       "3  {'numOutputRows': '6', 'numTargetRowsInserted'...         None   \n",
       "4  {'numOutputRows': '4', 'numTargetRowsInserted'...         None   \n",
       "5  {'numOutputRows': '4', 'numTargetRowsInserted'...         None   \n",
       "6  {'numOutputRows': '4', 'numTargetRowsInserted'...         None   \n",
       "7  {'numOutputRows': '4', 'numOutputBytes': '1852...         None   \n",
       "\n",
       "                            engineInfo  \n",
       "0  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "1  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "2  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "3  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "4  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "5  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "6  Apache-Spark/3.3.1 Delta-Lake/2.2.0  \n",
       "7  Apache-Spark/3.3.1 Delta-Lake/2.2.0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.sql('DESCRIBE HISTORY streaming_json.raw').toPandas()\n",
    "om = df['operationMetrics']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6bd5f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"numOutputRows\": \"6\",\n",
      "    \"numTargetRowsInserted\": \"2\",\n",
      "    \"numTargetRowsUpdated\": \"4\",\n",
      "    \"numTargetFilesAdded\": \"6\",\n",
      "    \"numTargetFilesRemoved\": \"4\",\n",
      "    \"numTargetRowsDeleted\": \"0\",\n",
      "    \"scanTimeMs\": \"230\",\n",
      "    \"numSourceRows\": \"6\",\n",
      "    \"numTargetChangeFilesAdded\": \"0\",\n",
      "    \"executionTimeMs\": \"455\",\n",
      "    \"numTargetRowsCopied\": \"0\",\n",
      "    \"rewriteTimeMs\": \"214\"\n",
      "}\n",
      "{\n",
      "    \"numOutputRows\": \"4\",\n",
      "    \"numTargetRowsInserted\": \"0\",\n",
      "    \"numTargetRowsUpdated\": \"4\",\n",
      "    \"numTargetFilesAdded\": \"4\",\n",
      "    \"numTargetFilesRemoved\": \"4\",\n",
      "    \"numTargetRowsDeleted\": \"0\",\n",
      "    \"scanTimeMs\": \"230\",\n",
      "    \"numSourceRows\": \"4\",\n",
      "    \"numTargetChangeFilesAdded\": \"0\",\n",
      "    \"executionTimeMs\": \"424\",\n",
      "    \"numTargetRowsCopied\": \"0\",\n",
      "    \"rewriteTimeMs\": \"186\"\n",
      "}\n",
      "{\n",
      "    \"numOutputRows\": \"4\",\n",
      "    \"numTargetRowsInserted\": \"2\",\n",
      "    \"numTargetRowsUpdated\": \"2\",\n",
      "    \"numTargetFilesAdded\": \"4\",\n",
      "    \"numTargetFilesRemoved\": \"2\",\n",
      "    \"numTargetRowsDeleted\": \"0\",\n",
      "    \"scanTimeMs\": \"267\",\n",
      "    \"numSourceRows\": \"4\",\n",
      "    \"numTargetChangeFilesAdded\": \"0\",\n",
      "    \"executionTimeMs\": \"501\",\n",
      "    \"numTargetRowsCopied\": \"0\",\n",
      "    \"rewriteTimeMs\": \"225\"\n",
      "}\n",
      "{\n",
      "    \"numOutputRows\": \"4\",\n",
      "    \"numTargetRowsInserted\": \"0\",\n",
      "    \"numTargetRowsUpdated\": \"4\",\n",
      "    \"numTargetFilesAdded\": \"4\",\n",
      "    \"numTargetFilesRemoved\": \"4\",\n",
      "    \"numTargetRowsDeleted\": \"0\",\n",
      "    \"scanTimeMs\": \"647\",\n",
      "    \"numSourceRows\": \"4\",\n",
      "    \"numTargetChangeFilesAdded\": \"0\",\n",
      "    \"executionTimeMs\": \"1041\",\n",
      "    \"numTargetRowsCopied\": \"0\",\n",
      "    \"rewriteTimeMs\": \"367\"\n",
      "}\n",
      "{\n",
      "    \"numOutputRows\": \"4\",\n",
      "    \"numOutputBytes\": \"1852\",\n",
      "    \"numFiles\": \"4\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for v in om:\n",
    "    print(json.dumps(v, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "35ab0bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>value</th>\n",
       "      <th>_change_type</th>\n",
       "      <th>_commit_version</th>\n",
       "      <th>_commit_timestamp</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-02-22 18:28:24.889</td>\n",
       "      <td>asdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-02-22 18:28:24.889</td>\n",
       "      <td>rewq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:46.091</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>insert</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-22 18:26:46.091</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3.14</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-22 18:26:57.964</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>3.14</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-22 18:26:57.964</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  value      _change_type  _commit_version       _commit_timestamp  \\\n",
       "0    5   1.00            insert                8 2023-02-22 18:28:24.889   \n",
       "1    6   1.00            insert                8 2023-02-22 18:28:24.889   \n",
       "2    1   1.00            insert                2 2023-02-22 18:26:46.091   \n",
       "3    2   1.00            insert                2 2023-02-22 18:26:46.091   \n",
       "4    3   3.14  update_postimage                3 2023-02-22 18:26:57.964   \n",
       "5    4   3.14  update_postimage                3 2023-02-22 18:26:57.964   \n",
       "\n",
       "  description  \n",
       "0        asdf  \n",
       "1        rewq  \n",
       "2        None  \n",
       "3        None  \n",
       "4        None  \n",
       "5        None  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format('delta').load('../gen_tiers/pyspark_tables/etl_tiers.db/bronze/')\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c3fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

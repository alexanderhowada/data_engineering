{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae18bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from raw_etl.ipynb\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, DoubleType, LongType, StructField, StructType\n",
    "\n",
    "import import_ipynb\n",
    "from raw_etl import RawETL, merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "442d70cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BronzeETL(RawETL):\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField('idx', LongType(), False),\n",
    "        StructField('value', DoubleType(), False),\n",
    "    ])\n",
    "\n",
    "    def extract(self, source_table, stg_table, spark_session=None):\n",
    "\n",
    "        df = self.spark.table(source_table)\n",
    "        \n",
    "        last_version = self.spark.sql(f'describe history {source_table}').toPandas().loc[0, 'version']\n",
    "        \n",
    "        stream = self.spark.readStream.format(\"delta\") \\\n",
    "             .option('delta.enableChangeDataFeed', True) \\\n",
    "             .option(\"readChangeFeed\", \"true\") \\\n",
    "             .option(\"startingVersion\", last_version) \\\n",
    "             .table(source_table)\n",
    "    \n",
    "        return stream\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \n",
    "        for c in self.schema.names:\n",
    "            df = df.withColumn(c, df[c].cast(self.schema[c].dataType))\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def merge(self, batch_df, batch_id):\n",
    "        # print('\\n inside batch \\n ')\n",
    "        # display(batch_df.toPandas())\n",
    "        \n",
    "        batch_df = batch_df.filter(\"_change_type = 'update_postimage' or _change_type = 'insert'\")\n",
    "        \n",
    "        merge(batch_df, self.target_table, self.pk, self.spark)\n",
    "    \n",
    "    def load(self, stream, target_table):\n",
    "        \n",
    "        self.target_table = target_table\n",
    "    \n",
    "        query = stream.writeStream \\\n",
    "              .format('delta') \\\n",
    "              .outputMode('update') \\\n",
    "              .foreachBatch(self.merge).start()\n",
    "        \n",
    "        stream.writeStream.format('delta').outputMode('append') \\\n",
    "              .option(\"checkpointLocation\", f\"./checkpoints/stream\") \\\n",
    "              .toTable(\"stream\")\n",
    "        #query.awaitTermination()\n",
    "        return query\n",
    "        \n",
    "        #merge(df, target_table, self.pk, spark_session=self.spark)\n",
    "        #self.spark.sql(f\"DROP TABLE {stg_table}\")\n",
    "        \n",
    "        \n",
    "    def etl(self, source_table, stg_table, target_table):\n",
    "        stream = self.extract(source_table, stg_table)\n",
    "        stream = self.transform(stream)\n",
    "        self.load(stream, target_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef17b1d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: delta.enableChangeDataFeed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/15 15:27:38 WARN Utils: Your hostname, spiriel resolves to a loopback address: 127.0.1.1; using 192.168.15.6 instead (on interface enp3s0)\n",
      "23/02/15 15:27:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/ahow/main_env/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ahow/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ahow/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0743322c-9090-4cf6-a48b-bbec83858df5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.2.0 in central\n",
      "\tfound io.delta#delta-storage;2.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 104ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0743322c-9090-4cf6-a48b-bbec83858df5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/15 15:27:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/15 15:27:45 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/15 15:27:49 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-20dfb7a4-1089-4ad3-9470-a4b14efad7db. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/02/15 15:27:49 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "\n",
      " inside batch \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>value</th>\n",
       "      <th>type</th>\n",
       "      <th>_change_type</th>\n",
       "      <th>_commit_version</th>\n",
       "      <th>_commit_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>3.14</td>\n",
       "      <td>range</td>\n",
       "      <td>update_preimage</td>\n",
       "      <td>113</td>\n",
       "      <td>2023-02-11 04:09:03.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0.00</td>\n",
       "      <td>range</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>113</td>\n",
       "      <td>2023-02-11 04:09:03.511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  value   type      _change_type  _commit_version  \\\n",
       "0  100   3.14  range   update_preimage              113   \n",
       "1  100   0.00  range  update_postimage              113   \n",
       "\n",
       "        _commit_timestamp  \n",
       "0 2023-02-11 04:09:03.511  \n",
       "1 2023-02-11 04:09:03.511  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/15 15:27:50 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    from pyspark.sql import SparkSession\n",
    "    from delta.pip_utils import configure_spark_with_delta_pip\n",
    "    \n",
    "    builder = SparkSession.builder\\\n",
    "           .appName('raw_etl')\\\n",
    "           .config('spark.sql.warehouse.dir', 'pyspark_tables')\\\n",
    "           .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "           .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "           .config('spark.databricks.delta.retentionDurationCheck.enabled', False) \\\n",
    "           .config('spark.databricks.delta.schema.autoMerge.enabled', True) \\\n",
    "           .config('spark.databricks.delta.checkLatestSchemaOnRead', True) \\\n",
    "           .config('delta.enableChangeDataFeed', True)\n",
    "\n",
    "    spark = configure_spark_with_delta_pip(builder).enableHiveSupport().getOrCreate()\n",
    "    \n",
    "    TARGET_TABLE = 'dummy.bronze'\n",
    "    SOURCE_TABLE = 'dummy.raw'\n",
    "    STG_TABLE = 'dummy.stg'\n",
    "    \n",
    "    etl = bronzeEtl(spark)\n",
    "    etl.etl(SOURCE_TABLE, STG_TABLE, TARGET_TABLE)#.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "375408b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|     72|2023-02-11 04:13:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         71|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     71|2023-02-11 04:08:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         70|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     70|2023-02-11 04:08:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         69|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     69|2023-02-11 04:06:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         68|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     68|2023-02-11 03:59:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         67|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     67|2023-02-11 03:50:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         66|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     66|2023-02-11 03:44:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         65|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     65|2023-02-10 05:13:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         64|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     64|2023-02-10 05:11:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         63|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     63|2023-02-10 05:09:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         62|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     62|2023-02-10 05:08:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         61|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     61|2023-02-10 05:07:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         60|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     60|2023-02-10 05:07:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         59|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     59|2023-02-10 05:06:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         58|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     58|2023-02-10 05:04:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         57|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     57|2023-02-10 05:04:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         56|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     56|2023-02-10 04:59:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         55|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     55|2023-02-10 04:56:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         54|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     54|2023-02-10 04:55:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         53|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|     53|2023-02-10 04:55:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|         52|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "numOutputRows 1\n",
      "numTargetRowsInserted 0\n",
      "numTargetRowsUpdated 1\n",
      "numTargetFilesAdded 1\n",
      "numTargetFilesRemoved 1\n",
      "numTargetRowsDeleted 0\n",
      "scanTimeMs 325\n",
      "numSourceRows 1\n",
      "numTargetChangeFilesAdded 0\n",
      "executionTimeMs 545\n",
      "numTargetRowsCopied 0\n",
      "rewriteTimeMs 204\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    }
   ],
   "source": [
    "# series = spark.sql('describe history dummy.bronze').show()\n",
    "# series = spark.sql('describe history dummy.bronze').toPandas()\n",
    "# series = series[series['version'] == series['version'].max() ]['operationMetrics']\n",
    "# for s in series:\n",
    "#     for k,v in s.items():\n",
    "#         print(k, v)\n",
    "#     print('\\n')\n",
    "#spark.sql('alter table dummy.bronze add partition (idx)').show()\n",
    "#spark.sql('ALTER TABLE dummy.raw SET TBLPROPERTIES (delta.enableChangeDataFeed=true)').toPandas()\n",
    "#df = spark.read.table('dummy.bronze')\n",
    "#display(df.toPandas())\n",
    "#df.write.partitionBy('idx').format('delta').mode('overwrite').saveAsTable('dummy.bronze_')\n",
    "\n",
    "#spark.sql('drop table dummy.bronze;').show()\n",
    "#spark.sql('alter table dummy.bronze_ rename to dummy.bronze;').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "455800c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>value</th>\n",
       "      <th>type</th>\n",
       "      <th>_change_type</th>\n",
       "      <th>_commit_version</th>\n",
       "      <th>_commit_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>3.14</td>\n",
       "      <td>range</td>\n",
       "      <td>update_preimage</td>\n",
       "      <td>113</td>\n",
       "      <td>2023-02-11 04:09:03.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0.00</td>\n",
       "      <td>range</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>113</td>\n",
       "      <td>2023-02-11 04:09:03.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>asdf</td>\n",
       "      <td>delete</td>\n",
       "      <td>112</td>\n",
       "      <td>2023-02-11 04:06:06.226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  value   type      _change_type  _commit_version  \\\n",
       "0  100   3.14  range   update_preimage              113   \n",
       "1  100   0.00  range  update_postimage              113   \n",
       "2    3   0.00   asdf            delete              112   \n",
       "\n",
       "        _commit_timestamp  \n",
       "0 2023-02-11 04:09:03.511  \n",
       "1 2023-02-11 04:09:03.511  \n",
       "2 2023-02-11 04:06:06.226  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql('select * from stream').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a53a2dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'numOutputRows': '1', 'numTargetRowsInserted': '0', 'numTargetRowsUpdated': '1', 'numTargetFilesAdded': '1', 'numTargetFilesRemoved': '1', 'numTargetRowsDeleted': '0', 'scanTimeMs': '401', 'numSourceRows': '1', 'numTargetChangeFilesAdded': '0', 'executionTimeMs': '596', 'numTargetRowsCopied': '0', 'rewriteTimeMs': '181'}\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     917|\n",
      "+--------+\n",
      "\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|  0.0|\n",
      "+-----+\n",
      "\n",
      "+---+---+\n",
      "|idx|  c|\n",
      "+---+---+\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(spark.sql('describe history dummy.bronze').toPandas()['operationMetrics'][0])\n",
    "# spark.sql(\"select count(1) from dummy.bronze\").show()\n",
    "# spark.sql('select value from dummy.bronze where idx=3').show()\n",
    "# spark.sql('select idx, count(1) c from dummy.bronze group by idx having c >1 ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bcd3240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " inside batch \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>value</th>\n",
       "      <th>type</th>\n",
       "      <th>_change_type</th>\n",
       "      <th>_commit_version</th>\n",
       "      <th>_commit_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>3.14</td>\n",
       "      <td>range</td>\n",
       "      <td>update_preimage</td>\n",
       "      <td>113</td>\n",
       "      <td>2023-02-11 04:09:03.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0.00</td>\n",
       "      <td>range</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>113</td>\n",
       "      <td>2023-02-11 04:09:03.511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  value   type      _change_type  _commit_version  \\\n",
       "0  100   3.14  range   update_preimage              113   \n",
       "1  100   0.00  range  update_postimage              113   \n",
       "\n",
       "        _commit_timestamp  \n",
       "0 2023-02-11 04:09:03.511  \n",
       "1 2023-02-11 04:09:03.511  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " inside batch \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>value</th>\n",
       "      <th>type</th>\n",
       "      <th>_change_type</th>\n",
       "      <th>_commit_version</th>\n",
       "      <th>_commit_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>3.14</td>\n",
       "      <td>range</td>\n",
       "      <td>update_preimage</td>\n",
       "      <td>113</td>\n",
       "      <td>2023-02-11 04:09:03.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0.00</td>\n",
       "      <td>range</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>113</td>\n",
       "      <td>2023-02-11 04:09:03.511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  value   type      _change_type  _commit_version  \\\n",
       "0  100   3.14  range   update_preimage              113   \n",
       "1  100   0.00  range  update_postimage              113   \n",
       "\n",
       "        _commit_timestamp  \n",
       "0 2023-02-11 04:09:03.511  \n",
       "1 2023-02-11 04:09:03.511  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " inside batch \n",
      " \n",
      "+-----------------+\n",
      "|num_affected_rows|\n",
      "+-----------------+\n",
      "|                1|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>value</th>\n",
       "      <th>type</th>\n",
       "      <th>_change_type</th>\n",
       "      <th>_commit_version</th>\n",
       "      <th>_commit_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>3.14</td>\n",
       "      <td>range</td>\n",
       "      <td>update_preimage</td>\n",
       "      <td>113</td>\n",
       "      <td>2023-02-11 04:09:03.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0.00</td>\n",
       "      <td>range</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>113</td>\n",
       "      <td>2023-02-11 04:09:03.511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  value   type      _change_type  _commit_version  \\\n",
       "0  100   3.14  range   update_preimage              113   \n",
       "1  100   0.00  range  update_postimage              113   \n",
       "\n",
       "        _commit_timestamp  \n",
       "0 2023-02-11 04:09:03.511  \n",
       "1 2023-02-11 04:09:03.511  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/11 04:09:05 ERROR MicroBatchExecution: Query [id = 89c6e5fa-bc5b-41eb-bdf0-8b267f9dd10b, runId = f29e9e28-e64e-4d03-9a05-20d93be18bf6] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 272, in call\n",
      "    raise e\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 269, in call\n",
      "    self.func(DataFrame(jdf, self.session), batch_id)\n",
      "  File \"/tmp/ipykernel_135192/2736145398.py\", line 34, in merge\n",
      "    merge(batch_df, self.target_table, self.pk, self.spark)\n",
      "  File \"<string>\", line 27, in merge\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/delta/tables.py\", line 938, in execute\n",
      "    self._jbuilder.execute()\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o523.execute.\n",
      ": org.apache.spark.sql.delta.DeltaUnsupportedOperationException: Cannot perform Merge as multiple source rows matched and attempted to modify the same\n",
      "target row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\n",
      "when multiple source rows match on the same target row, the result may be ambiguous\n",
      "as it is unclear which source row should be used to update or delete the matching\n",
      "target row. You can preprocess the source table to eliminate the possibility of\n",
      "multiple matches. Please refer to\n",
      "https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrors.scala:1108)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.multipleSourceRowMatchingTargetRowInMergeException$(DeltaErrors.scala:1105)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrors$.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrors.scala:2489)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$findTouchedFiles$1(MergeIntoCommand.scala:490)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordMergeOperation(MergeIntoCommand.scala:1004)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.findTouchedFiles(MergeIntoCommand.scala:426)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2(MergeIntoCommand.scala:367)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2$adapted(MergeIntoCommand.scala:340)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:252)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$1(MergeIntoCommand.scala:340)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordFrameProfile(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:132)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordOperation(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:131)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:121)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:109)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordDeltaOperation(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runMerge(MergeIntoCommand.scala:338)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$1(MergeIntoCommand.scala:333)\n",
      "\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:102)\n",
      "\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:90)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runWithMaterializedSourceLostRetries(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.run(MergeIntoCommand.scala:333)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:230)\n",
      "\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:105)\n",
      "\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:91)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:122)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:206)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy44.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat com.sun.proxy.$Proxy44.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'numAddedFiles': '1',\n",
       " 'scanTimeMs': '116',\n",
       " 'numCopiedRows': '0',\n",
       " 'executionTimeMs': '246',\n",
       " 'numAddedChangeFiles': '1',\n",
       " 'numUpdatedRows': '1',\n",
       " 'numRemovedFiles': '1',\n",
       " 'rewriteTimeMs': '130'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/11 04:09:05 ERROR MicroBatchExecution: Query [id = 3a0f897c-1526-4036-9169-9aaaf2db2d81, runId = 23cc923c-057e-4517-850d-bef16d482d9b] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 272, in call\n",
      "    raise e\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 269, in call\n",
      "    self.func(DataFrame(jdf, self.session), batch_id)\n",
      "  File \"/tmp/ipykernel_135192/2039092094.py\", line 35, in merge\n",
      "    merge(batch_df, self.target_table, self.pk, self.spark)\n",
      "  File \"<string>\", line 27, in merge\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/delta/tables.py\", line 938, in execute\n",
      "    self._jbuilder.execute()\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o539.execute.\n",
      ": org.apache.spark.sql.delta.DeltaUnsupportedOperationException: Cannot perform Merge as multiple source rows matched and attempted to modify the same\n",
      "target row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\n",
      "when multiple source rows match on the same target row, the result may be ambiguous\n",
      "as it is unclear which source row should be used to update or delete the matching\n",
      "target row. You can preprocess the source table to eliminate the possibility of\n",
      "multiple matches. Please refer to\n",
      "https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrors.scala:1108)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.multipleSourceRowMatchingTargetRowInMergeException$(DeltaErrors.scala:1105)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrors$.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrors.scala:2489)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$findTouchedFiles$1(MergeIntoCommand.scala:490)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordMergeOperation(MergeIntoCommand.scala:1004)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.findTouchedFiles(MergeIntoCommand.scala:426)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2(MergeIntoCommand.scala:367)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2$adapted(MergeIntoCommand.scala:340)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:252)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$1(MergeIntoCommand.scala:340)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordFrameProfile(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:132)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordOperation(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:131)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:121)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:109)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordDeltaOperation(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runMerge(MergeIntoCommand.scala:338)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$1(MergeIntoCommand.scala:333)\n",
      "\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:102)\n",
      "\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:90)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runWithMaterializedSourceLostRetries(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.run(MergeIntoCommand.scala:333)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:230)\n",
      "\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:105)\n",
      "\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:91)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:122)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:206)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy44.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat com.sun.proxy.$Proxy44.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "23/02/11 04:09:05 ERROR MicroBatchExecution: Query [id = fd2e6d9e-aba9-4c13-9209-ba0bae8dbef1, runId = facdb232-2c3d-4973-93d8-65beda37c2f5] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 272, in call\n",
      "    raise e\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 269, in call\n",
      "    self.func(DataFrame(jdf, self.session), batch_id)\n",
      "  File \"/tmp/ipykernel_135192/2039092094.py\", line 35, in merge\n",
      "    merge(batch_df, self.target_table, self.pk, self.spark)\n",
      "  File \"<string>\", line 27, in merge\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/delta/tables.py\", line 938, in execute\n",
      "    self._jbuilder.execute()\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/ahow/main_env/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o554.execute.\n",
      ": org.apache.spark.sql.delta.DeltaUnsupportedOperationException: Cannot perform Merge as multiple source rows matched and attempted to modify the same\n",
      "target row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\n",
      "when multiple source rows match on the same target row, the result may be ambiguous\n",
      "as it is unclear which source row should be used to update or delete the matching\n",
      "target row. You can preprocess the source table to eliminate the possibility of\n",
      "multiple matches. Please refer to\n",
      "https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrors.scala:1108)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.multipleSourceRowMatchingTargetRowInMergeException$(DeltaErrors.scala:1105)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrors$.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrors.scala:2489)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$findTouchedFiles$1(MergeIntoCommand.scala:490)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordMergeOperation(MergeIntoCommand.scala:1004)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.findTouchedFiles(MergeIntoCommand.scala:426)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2(MergeIntoCommand.scala:367)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2$adapted(MergeIntoCommand.scala:340)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:252)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$1(MergeIntoCommand.scala:340)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordFrameProfile(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:132)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordOperation(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:131)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:121)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:109)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordDeltaOperation(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runMerge(MergeIntoCommand.scala:338)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$1(MergeIntoCommand.scala:333)\n",
      "\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:102)\n",
      "\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:90)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runWithMaterializedSourceLostRetries(MergeIntoCommand.scala:221)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.run(MergeIntoCommand.scala:333)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:230)\n",
      "\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:105)\n",
      "\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:91)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:122)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:206)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy44.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat com.sun.proxy.$Proxy44.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n"
     ]
    }
   ],
   "source": [
    "# #vals = []\n",
    "# #for i in range(200, 1000):\n",
    "# #    vals.append(f'({i}, 3.14, \"range\")')\n",
    "# #spark.sql(f'insert into dummy.raw(idx, value, type) values {\",\".join(vals)}').show()\n",
    "# #spark.sql('update dummy.raw set value=0 where idx=3').show()\n",
    "# spark.sql('update dummy.raw set value=0 where idx=100').show()\n",
    "# #spark.sql('select * from dummy.raw').show()\n",
    "# spark.sql('describe history dummy.raw').toPandas()['operationMetrics'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52dc0375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql('select count(1) from stream').show()\n",
    "# spark.sql('drop table stream').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84e6a6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahow/main_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last_version = spark.sql('describe history dummy.raw').toPandas().loc[0, 'version']\n",
    "\n",
    "# spark.read.format(\"delta\") \\\n",
    "#   .option(\"readChangeFeed\", \"true\") \\\n",
    "#   .option(\"startingVersion\", last_version) \\\n",
    "#   .table(\"dummy.raw\").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
